{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"competition","sourceId":66653,"databundleVersionId":7500999},{"sourceType":"datasetVersion","sourceId":8047766,"datasetId":4743157,"databundleVersionId":8161627},{"sourceType":"datasetVersion","sourceId":7994788,"datasetId":4699615,"databundleVersionId":8106055},{"sourceType":"datasetVersion","sourceId":8060523,"datasetId":3516822,"databundleVersionId":8175429},{"sourceType":"datasetVersion","sourceId":7803679,"datasetId":4340749,"databundleVersionId":7906387},{"sourceType":"datasetVersion","sourceId":7784594,"datasetId":4555967,"databundleVersionId":7886478},{"sourceType":"datasetVersion","sourceId":7908837,"datasetId":4646023,"databundleVersionId":8016400}],"dockerImageVersionId":30683,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":417.718931,"end_time":"2024-04-07T08:28:09.335192","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-07T08:21:11.616261","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"0016ea3135504c14a7879f5447b9ed88":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf81aba5f6ae4e938b80829bac04ab58","placeholder":"​","style":"IPY_MODEL_4ba6f2d5b46c47c9b4ffdeea1c9046d9","value":" 10/10 [00:02&lt;00:00,  2.93it/s]"}},"00ef8135f65145808d5e19dc2840c72e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"01c60797aa384d8eb0a9c32ee039b35d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_301e2be27f144eb887f9fded657b80c6","placeholder":"​","style":"IPY_MODEL_6f4ffc2619eb4114bb61d6cf4ae0c3e0","value":"Predicting: 100%"}},"02580bcc62ad45f3b3946e8264f83b2e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"03054b25c9b44ef4a7579d98e2ca94d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae25cd341c564e028394bf6e4a32dc10","placeholder":"​","style":"IPY_MODEL_ac593bb427ba4a00a9511908f60141b5","value":"Tokenizing #1: 100%"}},"04f0ea8731d24c878878dbd43917480a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"091c48f13f224b18a6e97b1d246862c9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f47ed11dc7b4a9eb4b3cc5d01136025":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dc134a9dbc6b44d8948a70acb506085d","IPY_MODEL_7469e1f62a3e4305873a2a2c43980d41","IPY_MODEL_954abc3523f14d00a760695de5f0d021"],"layout":"IPY_MODEL_232d15f1115a4768b2137dfd1324a695"}},"12630ec7f4a34603b9573ef356c97b8b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13cdfccef7e94a73b39fa115b54a47d5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"143b83e377dc44439f36f23f3af42427":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15e39f3a53674f4a8577b20b45e19f6d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_85a5117486524ce195759601af262ab9","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_405aed64ccb34790ba0f8249c1a526a2","value":10}},"196499f2e6f44ffab09134dec9cd3fc6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12630ec7f4a34603b9573ef356c97b8b","placeholder":"​","style":"IPY_MODEL_c86604c80a254e4d9f70e9f7a47bb21c","value":"Predicting: 100%"}},"1dcdc14bf22648e492f6f15651ead7fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9084f5a973447e680b8fa040723e533","placeholder":"​","style":"IPY_MODEL_29e2abd814d747fc9dde3b4af6a1fcb4","value":" 10/10 [00:02&lt;00:00,  2.95it/s]"}},"1f8fc4426de340c1b0fb29875e9f04d7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"200631f6cbb748e4b9f44f32255e0a99":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"232d15f1115a4768b2137dfd1324a695":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"235edb5717064bee9a8836e212136bb2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_72fc9fb02e75411a993de489e4c3fe82","placeholder":"​","style":"IPY_MODEL_02580bcc62ad45f3b3946e8264f83b2e","value":" 5/5 [00:00&lt;00:00, 58.94ex/s]"}},"26e581d6d1fb46538a4fd9cfbfbb6378":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3905a8e5a8e54326888e3da9d561b88e","placeholder":"​","style":"IPY_MODEL_cf9e4bb27122490381a978d298e42513","value":" 10/10 [00:02&lt;00:00,  2.95it/s]"}},"271e67e5472249888c44c2de59ca7778":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29e2abd814d747fc9dde3b4af6a1fcb4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b0c4e8b6e414ba6969efc1767bfaea6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2de59a6ad70c4654a9dcf9c55ef2fcde":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bfe70f84aa884cbaac6f1bde02dee04d","placeholder":"​","style":"IPY_MODEL_d0ca31fc6d6f49eaa0591925d481d677","value":" 10/10 [00:02&lt;00:00,  2.93it/s]"}},"2fd69f62c7a240609a50eb7a8d04a8d8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"301e2be27f144eb887f9fded657b80c6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36e972aabd1e4d96a3175b7347a795bb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3905a8e5a8e54326888e3da9d561b88e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39bbfbfc5fad4b7ca7788df85f976941":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3ce78cfafd424165aa6645dee014fcec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_720f2a20b2a74c9e8fc7d7ab81e8339e","placeholder":"​","style":"IPY_MODEL_8706a81f161a4a69b9326e18227d3ee1","value":"Predicting: 100%"}},"405aed64ccb34790ba0f8249c1a526a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"48c178061c2647f6b570bd34e3121040":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9102905280744efbb342091286aab2bd","placeholder":"​","style":"IPY_MODEL_b407a5e6daa747cb8b0f7db520e3351c","value":"Tokenizing #0: 100%"}},"4a3f253fe789417babf6dd264d873404":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a8d21e873234bc2a44399179daa9d54":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_143b83e377dc44439f36f23f3af42427","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a650c17d6a5f498d96f9a99b6a58fc7d","value":10}},"4ba6f2d5b46c47c9b4ffdeea1c9046d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4be42f5e8b2f42649c3f71235acb03f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a6a577dd3a5f41baaf5119d9dbb9df9c","IPY_MODEL_15e39f3a53674f4a8577b20b45e19f6d","IPY_MODEL_26e581d6d1fb46538a4fd9cfbfbb6378"],"layout":"IPY_MODEL_ad0f53059be4439593fb96939e563e4f"}},"4d61452612234b45a72fefb7afd6bfbf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"52653bb752f14615a82e18bde6cde07b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"531438999a51400e8d0cc45504665c47":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e320d38865bd449299bbd8fe486fc356","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8995881b8b9f4fc1b57b4241ba6eb7e1","value":10}},"5443bdd981fe4511afd2204de8c1157f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f8fc4426de340c1b0fb29875e9f04d7","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4d61452612234b45a72fefb7afd6bfbf","value":10}},"5a156ca4320c42a0bfa52ec162ccca1d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8fc7f31db4e2443586202f3012ebd3b9","placeholder":"​","style":"IPY_MODEL_d1ae3ac446fa4ed692943bd316f3b607","value":"Predicting: 100%"}},"5d1064ed6e48431daae14e91ad3f63a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_03054b25c9b44ef4a7579d98e2ca94d7","IPY_MODEL_6968196adb09442e8ea5a6cbcc562659","IPY_MODEL_9bf16a36d04d4edea4ea55b54909b7ae"],"layout":"IPY_MODEL_ecb00a135f47456c9924cafe3c8a24b2"}},"5e9575752de640c2844f71c65902e8f6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac46f48ef6df410ca1a96c3dfaf94999","placeholder":"​","style":"IPY_MODEL_93e5a4d94beb4967b1b6a60b1e47602f","value":"Predicting: 100%"}},"60373ee184cd4c41887e7d2f857a20b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"652646d23f7f490bbe0552742b16a31a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65521c0f65c5435ab0fd43ff64defdf5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_48c178061c2647f6b570bd34e3121040","IPY_MODEL_b136a396fe634a1493918b0e2c72c5fe","IPY_MODEL_235edb5717064bee9a8836e212136bb2"],"layout":"IPY_MODEL_b6b9606fb8f04a7083edc838b7d64846"}},"6951d4d41bb44c4a90aaf07f093cbf35":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb8a52b94d624a7587a826d77b8cf674","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8712592fd30c44f489e3dfe09f42f843","value":10}},"6968196adb09442e8ea5a6cbcc562659":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_04f0ea8731d24c878878dbd43917480a","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_75ff9ca0318a479a95e5e48a7d152fe8","value":5}},"6deb13621ccf4443b9547bb8c6ae0945":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f4ffc2619eb4114bb61d6cf4ae0c3e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"720f2a20b2a74c9e8fc7d7ab81e8339e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72fc9fb02e75411a993de489e4c3fe82":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7469e1f62a3e4305873a2a2c43980d41":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_36e972aabd1e4d96a3175b7347a795bb","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_60373ee184cd4c41887e7d2f857a20b5","value":10}},"74f59b5ad19d4673b0e87d4f2f06287b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_091c48f13f224b18a6e97b1d246862c9","placeholder":"​","style":"IPY_MODEL_f9481c8f265a4886ac7815e39efb9db0","value":"Predicting: 100%"}},"75ff9ca0318a479a95e5e48a7d152fe8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"770614cefd374f14863bd9299dcaf47d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79cf591b635e45ca9e5a53f17e6cb418":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_271e67e5472249888c44c2de59ca7778","placeholder":"​","style":"IPY_MODEL_897832367c634749b0b4e64fb0b6e991","value":" 10/10 [00:02&lt;00:00,  2.95it/s]"}},"7c31f0a727a841aebc57c6375e9caf78":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5a156ca4320c42a0bfa52ec162ccca1d","IPY_MODEL_4a8d21e873234bc2a44399179daa9d54","IPY_MODEL_d8b390709cdb4732b0cd02ed5df70711"],"layout":"IPY_MODEL_652646d23f7f490bbe0552742b16a31a"}},"832f74acfd0547728111520f1d1b5df8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85a5117486524ce195759601af262ab9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8706a81f161a4a69b9326e18227d3ee1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8712592fd30c44f489e3dfe09f42f843":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"897832367c634749b0b4e64fb0b6e991":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8995881b8b9f4fc1b57b4241ba6eb7e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8da25bb5d99d49caac29fa02732d7419":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8fc7f31db4e2443586202f3012ebd3b9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9102905280744efbb342091286aab2bd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91f7df31d18d4e209435311e9b8f265e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9262449a273f41978a02119d37d9258b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_01c60797aa384d8eb0a9c32ee039b35d","IPY_MODEL_6951d4d41bb44c4a90aaf07f093cbf35","IPY_MODEL_79cf591b635e45ca9e5a53f17e6cb418"],"layout":"IPY_MODEL_f03ab9dbbf81482eaa095e8aea0a308d"}},"93e5a4d94beb4967b1b6a60b1e47602f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"954abc3523f14d00a760695de5f0d021":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_200631f6cbb748e4b9f44f32255e0a99","placeholder":"​","style":"IPY_MODEL_98aa357a5da34092a7252f82caae7779","value":" 10/10 [00:02&lt;00:00,  2.92it/s]"}},"98aa357a5da34092a7252f82caae7779":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9bf16a36d04d4edea4ea55b54909b7ae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6d7e008ba7e442082368b2b54c25657","placeholder":"​","style":"IPY_MODEL_b0395c381b824f6b8aa11579f99c20c8","value":" 5/5 [00:00&lt;00:00, 64.61ex/s]"}},"a2de67a0c99e43ba97b1737ebf0400f0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a650c17d6a5f498d96f9a99b6a58fc7d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a6a577dd3a5f41baaf5119d9dbb9df9c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_770614cefd374f14863bd9299dcaf47d","placeholder":"​","style":"IPY_MODEL_e1e1eb47d1c3408f9c8dba430d5eef36","value":"Predicting: 100%"}},"a6d7e008ba7e442082368b2b54c25657":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6f0059390b542d3b7a2729126907738":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa0ba46ba89946f898354b5e8f2de7ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5e9575752de640c2844f71c65902e8f6","IPY_MODEL_d64af605d8e148a5b3667a702e4bf4cf","IPY_MODEL_1dcdc14bf22648e492f6f15651ead7fa"],"layout":"IPY_MODEL_6deb13621ccf4443b9547bb8c6ae0945"}},"ac46f48ef6df410ca1a96c3dfaf94999":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac593bb427ba4a00a9511908f60141b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ad0f53059be4439593fb96939e563e4f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae25cd341c564e028394bf6e4a32dc10":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0395c381b824f6b8aa11579f99c20c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b136a396fe634a1493918b0e2c72c5fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_13cdfccef7e94a73b39fa115b54a47d5","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f73d310c392a4574b3bb317bbbf5665e","value":5}},"b407a5e6daa747cb8b0f7db520e3351c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b6b9606fb8f04a7083edc838b7d64846":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b764b5d96f0d4c09872834373fa2111e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3ce78cfafd424165aa6645dee014fcec","IPY_MODEL_531438999a51400e8d0cc45504665c47","IPY_MODEL_0016ea3135504c14a7879f5447b9ed88"],"layout":"IPY_MODEL_2fd69f62c7a240609a50eb7a8d04a8d8"}},"bfe70f84aa884cbaac6f1bde02dee04d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c80a47d097364b86931db532e2c9d9cd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_74f59b5ad19d4673b0e87d4f2f06287b","IPY_MODEL_5443bdd981fe4511afd2204de8c1157f","IPY_MODEL_f1661293a8524bddb38ffcbae422643b"],"layout":"IPY_MODEL_d54c8feac4ff489ba7c3403af379c3f1"}},"c86604c80a254e4d9f70e9f7a47bb21c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c8baf6f0f128486e8dbfa387805dc545":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb8a52b94d624a7587a826d77b8cf674":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf81aba5f6ae4e938b80829bac04ab58":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf9e4bb27122490381a978d298e42513":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d0ca31fc6d6f49eaa0591925d481d677":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d1ae3ac446fa4ed692943bd316f3b607":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d54c8feac4ff489ba7c3403af379c3f1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d64af605d8e148a5b3667a702e4bf4cf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_52653bb752f14615a82e18bde6cde07b","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8da25bb5d99d49caac29fa02732d7419","value":10}},"d8b390709cdb4732b0cd02ed5df70711":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91f7df31d18d4e209435311e9b8f265e","placeholder":"​","style":"IPY_MODEL_39bbfbfc5fad4b7ca7788df85f976941","value":" 10/10 [00:02&lt;00:00,  2.93it/s]"}},"da47fc4cf6c54dbc8feb134dc875a82d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_196499f2e6f44ffab09134dec9cd3fc6","IPY_MODEL_efce120554374cc18797ac1ea10439d8","IPY_MODEL_2de59a6ad70c4654a9dcf9c55ef2fcde"],"layout":"IPY_MODEL_a6f0059390b542d3b7a2729126907738"}},"dc134a9dbc6b44d8948a70acb506085d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2de67a0c99e43ba97b1737ebf0400f0","placeholder":"​","style":"IPY_MODEL_00ef8135f65145808d5e19dc2840c72e","value":"Predicting: 100%"}},"e1e1eb47d1c3408f9c8dba430d5eef36":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e320d38865bd449299bbd8fe486fc356":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9084f5a973447e680b8fa040723e533":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ecb00a135f47456c9924cafe3c8a24b2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efce120554374cc18797ac1ea10439d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_832f74acfd0547728111520f1d1b5df8","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4a3f253fe789417babf6dd264d873404","value":10}},"f03ab9dbbf81482eaa095e8aea0a308d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1661293a8524bddb38ffcbae422643b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8baf6f0f128486e8dbfa387805dc545","placeholder":"​","style":"IPY_MODEL_2b0c4e8b6e414ba6969efc1767bfaea6","value":" 10/10 [00:02&lt;00:00,  2.94it/s]"}},"f73d310c392a4574b3bb317bbbf5665e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f9481c8f265a4886ac7815e39efb9db0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Explained PII Inference\n### Introduction:\nWelcome to this Jupyter notebook developed for The Learning Agency Lab - PII Data Detection! This notebook is designed to help you participate in the competition and to Develop automated techniques to detect and remove PII from educational data.\n\n\n\n### Inspiration and Credits 🙌\nThis notebook is inspired by the work of Aleksandr Lavrikov, available at [this Kaggle project](https://www.kaggle.com/code/lavrikovav/0-968-to-onnx-30-200-speedup-pii-inference). I extend my gratitude to Aleksandr Lavrikov for sharing their insights and code publicly.\n\n\n### How to Use This Notebook:\n1. **Setup Environment**:\n   - Ensure all required libraries are installed. (Refer to the import libraries cell in the notebook for details.)\n   \n2. **Data Preparation**:\n   - Prepare your training and test datasets in JSON format. The paths to these datasets should be specified in the `config` class of the notebook.\n   - Optionally, you can downsample the training data for faster processing, specify the percentage in the `downsample` variable of the `config` class.\n   \n3. **Training and Evaluation**:\n   - Train the model using the provided training dataset by running the appropriate cells in the notebook. Adjust hyperparameters if necessary.\n   - Evaluate the trained model's performance on the test dataset to assess its effectiveness in detecting PII.\n\n4. **Inference**:\n   - Use the trained model to perform inference on new data. The notebook provides functionalities to tokenize input data, predict PII labels, and extract PII entities.\n\n5. **Export Results**:\n   - Export the processed predictions, including identified PII entities such as phone numbers, email addresses, URLs, etc., to a CSV file for further analysis or usage.\n\n**🌟 Explore my profile and other public projects, and don't forget to share your feedback!**\n\n## 👉 [Visit my Profile]( https://www.kaggle.com/code/zulqarnainalipk) 👈\n\n## How to Use 🛠️\nTo use this notebook effectively, please follow these steps:\n1. Ensure you have the competition data and environment set up.\n2. Execute each cell sequentially to perform data preparation, feature engineering, model training, and prediction submission.\n3. Customize and adapt the code as needed to improve model performance or experiment with different approaches.\n.\n\n## Acknowledgments 🙏\nI acknowledge The Learning Agency Lab organizers for providing the dataset and the competition platform.\n\nLet's get started! Feel free to reach out if you have any questions or need assistance along the way.\n👉 [Visit my Profile](https://www.kaggle.com/zulqarnainalipk) 👈\n","metadata":{}},{"cell_type":"markdown","source":"# 📁 Install & Import libraries\n","metadata":{}},{"cell_type":"code","source":"import os                      # 📁 Importing operating system module for file and directory operations\n\nimport subprocess\n\n# Suppress warnings and errors by redirecting output to /dev/null\nsubprocess.run([\"pip\", \"install\", \"-q\", \"/kaggle/input/onyx-runtime-gpu-whl/onnxruntime_gpu-1.17.1-cp310-cp310-manylinux_2_28_x86_64.whl\", \"--force-reinstall\", \"--no-index\", \"--find-links=/kaggle/input/onyx-runtime-gpu-whl\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\nsubprocess.run([\"pip\", \"install\", \"-q\", \"/kaggle/input/onyx-runtime-gpu-whl/onnx-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", \"--no-index\", \"--find-links=/kaggle/input/onyx-runtime-gpu-whl\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\nsubprocess.run([\"pip\", \"install\", \"-q\", \"/kaggle/input/onyx-runtime-gpu-whl/onnxconverter_common-1.14.0-py2.py3-none-any.whl\", \"--no-index\", \"--find-links=/kaggle/input/onyx-runtime-gpu-whl\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n\nimport gc                      # 🗑️ Importing garbage collection module for memory management\nfrom tqdm.auto import tqdm    # 🔄 Importing tqdm for progress bars\nimport json                    # 📝 Importing JSON module for JSON manipulation\nimport numpy as np             # 🔢 Importing NumPy for numerical operations\nimport pandas as pd            # 📊 Importing Pandas for data manipulation\nfrom itertools import chain   # 🔗 Importing itertools for iterating over data structures\nfrom text_unidecode import unidecode  # 📝 Importing text_unidecode for text normalization\nfrom typing import Dict, List, Tuple   # ✍️ Importing typing for type hinting\nimport codecs                  # 🧾 Importing codecs for file encoding and decoding\nfrom datasets import Dataset, load_from_disk   # 📦 Importing datasets module for working with datasets\nfrom sklearn.metrics import log_loss   # 📏 Importing log_loss from sklearn.metrics\nimport torch                   # 🔥 Importing PyTorch for deep learning\nimport torch.nn as nn          # 🧠 Importing nn module from PyTorch for neural network layers\nimport torch.nn.functional as F   # ➕ Importing F module from PyTorch for functional operations\nfrom torch.utils.data import DataLoader   # 📦 Importing DataLoader from PyTorch for loading data\nimport pickle                  # 🥒 Importing pickle for object serialization\nimport re                      # 🔍 Importing re for regular expressions\nfrom transformers import TrainingArguments, AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification   # 🤖 Importing transformers for working with pre-trained models\nfrom scipy.special import softmax   # 📊 Importing softmax from scipy.special for softmax calculation\nfrom spacy.lang.en import English   # 🌐 Importing English language model from spaCy for NLP\n\n# Toggle to use training set folds for model\ndebug_on_train_df = False   # 🔍 Setting debug_on_train_df to False to toggle using training set folds\n\n# Enable to convert models for inference on-the-fly.\nconvert_before_inference = False   # ⚙️ Setting convert_before_inference to False to toggle model conversion for inference on-the-fly\n\n# Temporary directory for saving datasets and intermediate files.\ntemp_data_folder = \"/tmp/output/\"   # 📁 Setting temp_data_folder to \"/tmp/output/\" for storing temporary data\n","metadata":{"id":"cfba0c61","papermill":{"duration":20.610872,"end_time":"2024-04-07T08:22:52.247576","exception":false,"start_time":"2024-04-07T08:22:31.636704","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T17:03:18.145349Z","iopub.execute_input":"2024-04-08T17:03:18.146123Z","iopub.status.idle":"2024-04-08T17:04:11.570761Z","shell.execute_reply.started":"2024-04-08T17:03:18.146090Z","shell.execute_reply":"2024-04-08T17:04:11.569725Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# 🔧 Configuration ","metadata":{"id":"9f6c9a20","papermill":{"duration":0.009477,"end_time":"2024-04-07T08:22:52.267255","exception":false,"start_time":"2024-04-07T08:22:52.257778","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"\n\nConfiguration class named `config` that holds various settings and paths required for training a Personal Identifiable Information (PII) detection model.\n\n🔍 **Explanation:**\n\n- **`device`**: Specifies the device where the model will be executed. Here it's set to `'cpu'`.\n- **`seed`**: Sets the random seed for reproducibility of results.\n- **`train_dataset_path`**: Path to the training dataset in JSON format.\n- **`test_dataset_path`**: Path to the test dataset in JSON format.\n- **`sample_submission_path`**: Path to the sample submission CSV file.\n- **`save_dir`**: Directory path for saving intermediate files.\n- **`downsample`**: Percentage of data to downsample during tokenization.\n- **`truncation`**: Whether to truncate sequences during tokenization.\n- **`padding`**: Specifies the padding strategy during tokenization.\n- **`max_length`**: Maximum sequence length after tokenization.\n- **`doc_stride`**: The stride length for splitting long documents into shorter chunks.\n- **`target_cols`**: List of target columns for the model.\n- **`load_from_disk`**: Flag indicating whether to load data from disk.\n- **`learning_rate`**: Learning rate for training the model.\n- **`batch_size`**: Batch size for training.\n- **`epochs`**: Number of training epochs.\n- **`NFOLDS`**: List of folds for cross-validation.\n- **`trn_fold`**: Index of the training fold.\n- **`model_paths`**: Dictionary containing paths to pretrained models and their corresponding weights.\n- **`converted_path`**: Path to the directory containing converted model files.\n\n📚 **Study Sources:**\n\n1. PyTorch Documentation: [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html)\n2. Transformers Documentation: [https://huggingface.co/transformers/index.html](https://huggingface.co/transformers/index.html)\n","metadata":{}},{"cell_type":"code","source":"class config:\n    # Specifies the device for running the model, which is set to 'cpu' indicating CPU.\n    device = 'cpu' \n    # Sets the random seed for reproducibility.\n    seed = 69\n    # Path to the training dataset in JSON format.\n    train_dataset_path = \"/kaggle/input/pii-detection-removal-from-educational-data/train.json\"\n    # Path to the test dataset in JSON format.\n    test_dataset_path = \"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"\n    # Path to the sample submission CSV file.\n    sample_submission_path = \"/home/nischay/PID/Data/sample_submission.csv\"\n       \n    # Directory path for saving intermediate files. It uses temp_data_folder which should be defined elsewhere in the code.\n    save_dir = temp_data_folder + \"1/\"\n\n    # Percentage of data to downsample during tokenization.\n    downsample = 0.45\n    # Whether to truncate sequences during tokenization.\n    truncation = True \n    # Specifies the padding strategy during tokenization, which is set to False indicating no padding.\n    padding = False #'max_length'\n    # Maximum sequence length after tokenization.\n    max_length = 3574\n    # The stride length for splitting long documents into shorter chunks.\n    doc_stride = 512\n    \n    # List of target columns for the model.\n    target_cols = ['B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', \n    'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', \n    'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL','O']\n\n    # Flag indicating whether to load data from disk.\n    load_from_disk = None\n\n    # Learning rate for training the model.\n    learning_rate = 1e-5\n    # Batch size for training.\n    batch_size = 1\n    # Number of training epochs.\n    epochs = 4\n    # List of folds for cross-validation.\n    NFOLDS = [0]\n    # Index of the training fold.\n    trn_fold = 0\n\n    # Dictionary containing paths to pretrained models and their corresponding weights.\n    model_paths = {\n    '/kaggle/input/37vp4pjt': 10/10,\n    '/kaggle/input/pii-deberta-models/cuerpo-de-piiranha': 2/10,\n    '/kaggle/input/pii-deberta-models/cola del piinguuino' : 1/10,\n    '/kaggle/input/pii-deberta-models/cabeza-del-piinguuino': 5/10,\n    '/kaggle/input/pii-deberta-models/cabeza-de-piiranha': 3/10,\n    '/kaggle/input/pii-deberta-models/cola-de-piiranha':1/10,\n    '/kaggle/input/pii-models/piidd-org-sakura': 2/10,\n    '/kaggle/input/pii-deberta-models/cabeza-de-piiranha-persuade_v0':1/10,\n    }\n    # Path to the directory containing converted model files.\n    converted_path = '/kaggle/input/toonnx2-converted-models'\n","metadata":{"id":"a824cb8b","outputId":"4e679198-2431-4060-86d2-fccce963ba3e","papermill":{"duration":0.020319,"end_time":"2024-04-07T08:22:52.297362","exception":false,"start_time":"2024-04-07T08:22:52.277043","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T17:04:11.572900Z","iopub.execute_input":"2024-04-08T17:04:11.573605Z","iopub.status.idle":"2024-04-08T17:04:11.583336Z","shell.execute_reply.started":"2024-04-08T17:04:11.573566Z","shell.execute_reply":"2024-04-08T17:04:11.582475Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"🔍 **Explanation:**\n\n- Checks whether the directory specified by `config.save_dir` exists or not.\n- If the directory does not exist, it creates the directory along with any necessary parent directories using `os.makedirs()`.\n- This ensures that the directory is available for saving intermediate files or any other purposes specified by the configuration.\n\n📚 **Study Sources:**\n\n1. Python Documentation - `os.makedirs()`: [https://docs.python.org/3/library/os.html#os.makedirs](https://docs.python.org/3/library/os.html#os.makedirs)\n2. Real Python - Understanding `os.makedirs()`: [https://realpython.com/python-os-mkdir/#creating-a-directory-tree](https://realpython.com/python-os-mkdir/#creating-a-directory-tree)","metadata":{}},{"cell_type":"code","source":"# Check if the directory specified by 'save_dir' in the configuration exists.\nif not os.path.exists(config.save_dir):\n    # If the directory doesn't exist, create it along with any necessary parent directories.\n    os.makedirs(config.save_dir)\n","metadata":{"id":"B1s0DROkC5p_","papermill":{"duration":0.016683,"end_time":"2024-04-07T08:22:52.323704","exception":false,"start_time":"2024-04-07T08:22:52.307021","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T17:04:11.584555Z","iopub.execute_input":"2024-04-08T17:04:11.584811Z","iopub.status.idle":"2024-04-08T17:04:11.596664Z","shell.execute_reply.started":"2024-04-08T17:04:11.584788Z","shell.execute_reply":"2024-04-08T17:04:11.595762Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"\n#  Natural Language Processing Setup 🧠\n🔍 **Explanation:**\n\n- **`nlp = English()`**: Initializes the English language model for natural language processing using SpaCy.\n- **`INFERENCE_MAX_LENGTH = 3500`**: Defines the maximum length for inference sequences. This is used to limit the length of processed text during inference.\n- **`threshold = 0.99`**: Sets the threshold for confidence score in detecting PII entities. Any entity prediction with a confidence score above this threshold is considered significant.\n- **Regular Expressions**:\n  - **`email_regex`**: Matches email addresses in text.\n  - **`phone_num_regex`**: Matches phone numbers in various formats.\n  - **`url_regex`**: Matches URLs in text.\n  - **`street_regex`**: Matches street addresses in text.\n\n📚 **Study Sources:**\n\n1. SpaCy Documentation: [https://spacy.io/](https://spacy.io/)\n2. Regular Expressions in Python: [https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html)","metadata":{}},{"cell_type":"code","source":"# Initializing the English language model for natural language processing.\nnlp = English()\n\n# Maximum length for inference sequences.\nINFERENCE_MAX_LENGTH = 3500\n\n# Threshold for confidence score in detecting PII entities.\nthreshold = 0.99\n\n# Regular expressions for detecting various types of PII.\nemail_regex = re.compile(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+')  # Email addresses\nphone_num_regex = re.compile(r\"(\\(\\d{3}\\)\\d{3}\\-\\d{4}\\w*|\\d{3}\\.\\d{3}\\.\\d{4})\\s\")  # Phone numbers\nurl_regex = re.compile(\n    r'http[s]?://'  # http or https\n    r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|'  # domain...\n    r'localhost|'  # localhost...\n    r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})'  # ...or ip\n    r'(?::\\d+)?'  # optional port\n    r'(?:/?|[/?]\\S+)', re.IGNORECASE)  # URLs\nstreet_regex = re.compile(r'\\d{1,4} [\\w\\s]{1,20}(?:street|apt|st|avenue|ave|road|rd|highway|hwy|square|sq|trail|trl|drive|dr|court|ct|parkway|pkwy|circle|cir|boulevard|blvd)\\W?(?=\\s|$)', re.IGNORECASE)  # Street addresses\n","metadata":{"papermill":{"duration":0.301504,"end_time":"2024-04-07T08:22:52.634936","exception":false,"start_time":"2024-04-07T08:22:52.333432","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T17:04:11.599195Z","iopub.execute_input":"2024-04-08T17:04:11.599556Z","iopub.status.idle":"2024-04-08T17:04:11.807600Z","shell.execute_reply.started":"2024-04-08T17:04:11.599530Z","shell.execute_reply":"2024-04-08T17:04:11.806833Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# 🔍🎯 Finding Spans in Document 🕵️‍♂️📜\n\n🔍 **Explanation:**\n\n- **`find_span` Function**:\n  - This function takes two arguments: `target`, a list of strings representing the target sequence to be found, and `document`, a list of strings representing the document to search within.\n  - Returns a list of lists, where each inner list contains the indices of the start and end positions of the found spans in the document.\n- **Variables**:\n  - **`idx`**: Tracks the current index in the `target` sequence.\n  - **`spans`**: Stores the list of spans found in the document.\n  - **`span`**: Stores the current span being constructed.\n- **Iteration**:\n  - Iterates through each token in the `document`.\n- **Matching**:\n  - If the current token matches the current target token, it appends the index to the `span` list.\n- **Completion**:\n  - When the entire target sequence is found, the span is appended to `spans` and the process continues to search for the next occurrence.\n- **Return**:\n  - Returns the list of spans found in the document.\n\n📚 **Study Sources:**\n\n1. Python Documentation - `enumerate()`: [https://docs.python.org/3/library/functions.html#enumerate](https://docs.python.org/3/library/functions.html#enumerate)","metadata":{}},{"cell_type":"code","source":"def find_span(target: list[str], document: list[str]) -> list[list[int]]:\n\n    # Initialize variables\n    idx = 0\n    spans = []\n    span = []\n\n    # Iterate through the document tokens\n    for i, token in enumerate(document):\n        # If the current token doesn't match the target start anew\n        if token != target[idx]:\n            idx = 0\n            span = []\n            continue\n        # If token matches, append its index to the span list\n        span.append(i)\n        idx += 1\n        # If the entire target is found, append the span to the list of spans\n        if idx == len(target):\n            spans.append(span)\n            # Reset span and idx for next potential match\n            span = []\n            idx = 0\n            continue\n    \n    return spans\n","metadata":{"papermill":{"duration":0.020271,"end_time":"2024-04-07T08:22:52.697146","exception":false,"start_time":"2024-04-07T08:22:52.676875","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T17:04:11.808689Z","iopub.execute_input":"2024-04-08T17:04:11.808986Z","iopub.status.idle":"2024-04-08T17:04:11.815888Z","shell.execute_reply.started":"2024-04-08T17:04:11.808962Z","shell.execute_reply":"2024-04-08T17:04:11.814900Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Load the training dataset from the JSON file specified in the configuration.\ndata = json.load(open(config.train_dataset_path))\n\n# Load the test dataset from the JSON file specified in the configuration.\ntest_data = json.load(open(config.test_dataset_path))\n\n","metadata":{"papermill":{"duration":2.572941,"end_time":"2024-04-07T08:22:55.279935","exception":false,"start_time":"2024-04-07T08:22:52.706994","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T17:04:11.817028Z","iopub.execute_input":"2024-04-08T17:04:11.817323Z","iopub.status.idle":"2024-04-08T17:04:13.670395Z","shell.execute_reply.started":"2024-04-08T17:04:11.817298Z","shell.execute_reply":"2024-04-08T17:04:13.669381Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"🔍 **Explanation:**\n\n- **`all_labels`**:\n  - Extracts all unique labels from the training data by iterating through each sample and accessing the \"labels\" key. \n  - The labels are flattened into a single list using `chain(*...)`.\n  - The list is converted to a set to remove duplicates and then sorted.\n- **`label2id` Dictionary**:\n  - Maps each unique label to its corresponding index in a dictionary comprehension.\n- **`id2label` Dictionary**:\n  - Maps each index to its corresponding label in a dictionary comprehension, providing a reverse mapping.\n\n","metadata":{}},{"cell_type":"code","source":"# Extract all unique labels from the training data and sort them.\nall_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n\n# Create a dictionary mapping each label to its corresponding index.\nlabel2id = {l: i for i,l in enumerate(all_labels)}\n\n# Create a dictionary mapping each index to its corresponding label.\nid2label = {v:k for k,v in label2id.items()}\n","metadata":{"id":"9e209155","papermill":{"duration":0.088666,"end_time":"2024-04-07T08:22:55.378905","exception":false,"start_time":"2024-04-07T08:22:55.290239","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T17:04:13.671680Z","iopub.execute_input":"2024-04-08T17:04:13.672067Z","iopub.status.idle":"2024-04-08T17:04:13.749279Z","shell.execute_reply.started":"2024-04-08T17:04:13.672034Z","shell.execute_reply":"2024-04-08T17:04:13.748292Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"\n# 🤖📝 Model Tokenizer Initialization 🤖🔤\n\n🔍 **Explanation:**\n\n- **`first_model_path`**:\n  - Retrieves the path of the first model from the dictionary of model paths specified in the configuration.\n- **Tokenizer Initialization**:\n  - The tokenizer is initialized using the `AutoTokenizer.from_pretrained()` method from the Hugging Face Transformers library.\n  - This method automatically selects the appropriate tokenizer based on the provided model path.\n\n📚 **Study Sources:**\n\n1. Hugging Face Transformers Documentation - Tokenizers: [https://huggingface.co/transformers/main_classes/tokenizer.html](https://huggingface.co/transformers/main_classes/tokenizer.html)","metadata":{}},{"cell_type":"code","source":"# Select the path of the first model from the configuration's model paths.\nfirst_model_path = list(config.model_paths.keys())[0]\n\n# Initialize the tokenizer using the AutoTokenizer class from the Hugging Face Transformers library.\ntokenizer = AutoTokenizer.from_pretrained(first_model_path)\n","metadata":{"id":"VHFHbC_3D4zg","outputId":"da6c707f-589d-4439-8d31-0e12f780455e","papermill":{"duration":1.072423,"end_time":"2024-04-07T08:22:56.461241","exception":false,"start_time":"2024-04-07T08:22:55.388818","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T17:04:13.750584Z","iopub.execute_input":"2024-04-08T17:04:13.750901Z","iopub.status.idle":"2024-04-08T17:04:14.598033Z","shell.execute_reply.started":"2024-04-08T17:04:13.750875Z","shell.execute_reply":"2024-04-08T17:04:14.597003Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Create a DataFrame for the training data using the loaded training data.\ndf_train = pd.DataFrame(data)\n\n# Add a new column 'fold' to the training DataFrame, representing the fold number.\ndf_train['fold'] = df_train['document'] % 4\n\n# Create a DataFrame for the test data using the loaded test data.\ndf_test = pd.DataFrame(test_data)\n","metadata":{"papermill":{"duration":0.060896,"end_time":"2024-04-07T08:22:56.532684","exception":false,"start_time":"2024-04-07T08:22:56.471788","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T17:04:14.599564Z","iopub.execute_input":"2024-04-08T17:04:14.599879Z","iopub.status.idle":"2024-04-08T17:04:14.842321Z","shell.execute_reply.started":"2024-04-08T17:04:14.599852Z","shell.execute_reply":"2024-04-08T17:04:14.841366Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# 📉🔢 DataFrame Downsampling Function 🛠️\n\n🔍 **Explanation:**\n\n- **`downsample_df` Function**:\n  - Takes a DataFrame `train_df` and a percentage `percent` as input and returns a downsampled DataFrame.\n- **`train_df['is_labels']`**:\n  - Adds a new column `'is_labels'` to the DataFrame indicating whether labels are present in each sample.\n  - Checks if any label in the `'labels'` column is not equal to `'O'`, indicating the presence of labels.\n- **Separating Samples**:\n  - Samples with labels (`true_samples`) and samples without labels (`false_samples`) are separated based on the value of the `'is_labels'` column.\n- **Downsampling False Samples**:\n  - The number of false samples to keep after downsampling is calculated based on the specified percentage.\n  - Random false samples are sampled without replacement to downsample them using `sample()` method.\n- **Concatenating DataFrames**:\n  - The true samples and downsampled false samples are concatenated using `pd.concat()` to create the downsampled DataFrame.\n- **Return**:\n  - The downsampled DataFrame is returned.\n\n📚 **Study Sources:**\n\n1. pandas Documentation - DataFrame: [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)\n2. pandas Documentation - `sample()`: [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html)","metadata":{}},{"cell_type":"code","source":"def downsample_df(train_df, percent):\n    # Add a new column 'is_labels' to indicate if labels are present in the sample\n    train_df['is_labels'] = train_df['labels'].apply(lambda labels: any(label != 'O' for label in labels))\n    \n    # Separate samples with labels and samples without labels\n    true_samples = train_df[train_df['is_labels'] == True]\n    false_samples = train_df[train_df['is_labels'] == False]\n    \n    # Calculate the number of false samples to keep after downsampling\n    n_false_samples = int(len(false_samples) * percent)\n    \n    # Randomly sample false samples to downsample\n    downsampled_false_samples = false_samples.sample(n=n_false_samples, random_state=42)\n    \n    # Concatenate true samples and downsampled false samples to create the downsampled DataFrame\n    downsampled_df = pd.concat([true_samples, downsampled_false_samples])\n    \n    return downsampled_df\n","metadata":{"papermill":{"duration":0.019311,"end_time":"2024-04-07T08:22:56.650191","exception":false,"start_time":"2024-04-07T08:22:56.630880","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T17:04:14.846345Z","iopub.execute_input":"2024-04-08T17:04:14.846657Z","iopub.status.idle":"2024-04-08T17:04:14.853999Z","shell.execute_reply.started":"2024-04-08T17:04:14.846630Z","shell.execute_reply":"2024-04-08T17:04:14.853110Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"\n# 🔠 Tokenization Function for DataFrame📄\n\n\n🔍 **Explanation:**\n\n- **`tokenize_row` Function**:\n  - Function takes a single row (`example`) from a DataFrame as input and tokenizes its text using the provided tokenizer.\n- **Initialization**:\n  - Initializes empty lists `text` and `token_map` to store the tokenized text and token map respectively.\n- **Tokenization Process**:\n  - Iterates through each token (`t`) and trailing whitespace (`ws`) in the example.\n  - For each token, it appends the token to the `text` list and extends the `token_map` with the index of the token repeated by its length.\n  - If trailing whitespace is present, it appends a space to the `text` and -1 to the `token_map`.\n- **Tokenization with Transformers Tokenizer**:\n  - Tokenizes the concatenated text using the provided tokenizer (`tokenizer`), considering specified configurations like truncation and maximum length.\n- **Return**:\n  - Returns a dictionary containing tokenized inputs (`input_ids`), attention mask (`attention_mask`), offset mappings (`offset_mapping`), and token map (`token_map`).\n\n📚 **Study Sources:**\n\n1. Hugging Face Transformers Documentation - Tokenizers: [https://huggingface.co/transformers/main_classes/tokenizer.html](https://huggingface.co/transformers/main_classes/tokenizer.html)\n2. Python Documentation - `zip()`: [https://docs.python.org/3/library/functions.html#zip](https://docs.python.org/3/library/functions.html#zip)","metadata":{}},{"cell_type":"code","source":"def tokenize_row(example):\n    # Initialize empty lists to store tokenized text and token map\n    text = []\n    token_map = []\n    \n    idx = 0\n    \n    # Iterate through tokens and trailing whitespaces in the example\n    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n        # Append token to the text list\n        text.append(t)\n        # Extend token map with index of token repeated by its length\n        token_map.extend([idx]*len(t))\n        # If trailing whitespace is present, append space to text and -1 to token map\n        if ws:\n            text.append(\" \")\n            token_map.append(-1)\n            \n        idx += 1\n        \n    # Tokenize the concatenated text using the tokenizer with specified configurations\n    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=config.truncation, max_length=config.max_length)\n    \n    # Return dictionary containing tokenized inputs, attention mask, offset mappings, and token map\n    return {\n        \"input_ids\": tokenized.input_ids,\n        \"attention_mask\": tokenized.attention_mask,\n        \"offset_mapping\": tokenized.offset_mapping,\n        \"token_map\": token_map,\n    }\n","metadata":{"papermill":{"duration":0.019508,"end_time":"2024-04-07T08:22:56.680244","exception":false,"start_time":"2024-04-07T08:22:56.660736","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T17:04:14.855004Z","iopub.execute_input":"2024-04-08T17:04:14.855283Z","iopub.status.idle":"2024-04-08T17:04:14.863211Z","shell.execute_reply.started":"2024-04-08T17:04:14.855251Z","shell.execute_reply":"2024-04-08T17:04:14.862257Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"\n🔍 **Explanation:**\n\n- **Debugging Enabled**:\n  - If debugging is enabled (`debug_on_train_df` is True), the code processes the training DataFrame for each fold.\n  - It subsets the DataFrame based on the fold, performs downsampling if configured, tokenizes rows, and saves the dataset and DataFrame to disk.\n- **Debugging Disabled**:\n  - If debugging is disabled, the code processes the test DataFrame.\n  - It tokenizes rows and saves the test dataset to disk.\n- **Data Loading from Disk**:\n  - The code checks if data loading from disk is disabled (`config.load_from_disk is None`).\n- **Tokenization and Saving**:\n  - The DataFrame is converted to a Hugging Face Dataset, tokenized using the `tokenize_row` function, and saved to disk.\n\n","metadata":{}},{"cell_type":"code","source":"if debug_on_train_df:\n\n    # Check if data loading from disk is disabled\n    if config.load_from_disk is None:\n        \n        # Add a new column 'fold' to the training DataFrame to indicate fold number\n        df_train['fold'] = df_train['document'] % 4\n        df_train.head(3)\n        \n        # Loop through different folds\n        for i in range(-1, 4):    \n            # Subset the training DataFrame for the current fold\n            train_df = df_train[df_train['fold']==i].reset_index(drop=True)\n\n            # Set valid_stride flag based on current fold\n            if i==config.trn_fold:\n                config.valid_stride = True\n            if i!=config.trn_fold and config.downsample > 0:\n                train_df = downsample_df(train_df, config.downsample)\n                config.valid_stride = False\n\n            train_df = train_df\n            print(len(train_df))\n            \n            # Convert DataFrame to Hugging Face Dataset and tokenize rows\n            ds = Dataset.from_pandas(train_df)\n            ds = ds.map(\n              tokenize_row,\n              batched=False,\n              num_proc=2,\n              desc=\"Tokenizing\",\n            )\n\n            # Save the dataset and DataFrame to disk\n            ds.save_to_disk(f\"{config.save_dir}fold_{i}.dataset\")\n            with open(f\"{config.save_dir}_pkl\", \"wb\") as fp:\n                pickle.dump(train_df, fp)\n            print(\"Saving dataset to disk:\", config.save_dir)\n        \n# If debugging is not enabled, process the test DataFrame\nelse:\n    \n    # Check if data loading from disk is disabled\n    if config.load_from_disk is None:\n\n        # Set valid_stride flag for test data\n        config.valid_stride = True\n        print(len(df_test))\n\n        # Convert test DataFrame to Hugging Face Dataset and tokenize rows\n        ds = Dataset.from_pandas(df_test)\n        ds = ds.map(\n          tokenize_row,\n          batched=False,\n          num_proc=2,\n          desc=\"Tokenizing\",\n        )\n\n        # Save the test dataset to disk\n        ds.save_to_disk(f\"{config.save_dir}test.dataset\")\n        print(\"Saving dataset to disk:\", config.save_dir)\n","metadata":{"id":"HMKUgZqmHIYv","outputId":"c1d9603a-47a5-4e66-9778-82c9d7740782","papermill":{"duration":0.444526,"end_time":"2024-04-07T08:22:57.170297","exception":false,"start_time":"2024-04-07T08:22:56.725771","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T17:04:14.864540Z","iopub.execute_input":"2024-04-08T17:04:14.864930Z","iopub.status.idle":"2024-04-08T17:04:15.421290Z","shell.execute_reply.started":"2024-04-08T17:04:14.864899Z","shell.execute_reply":"2024-04-08T17:04:15.420195Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Tokenizing (num_proc=2):   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"116922f1e6944052aedb069ec9d864c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"848a1d84b3ec42c4a86923eeca5079d3"}},"metadata":{}},{"name":"stdout","text":"Saving dataset to disk: /tmp/output/1/\n","output_type":"stream"}]},{"cell_type":"code","source":"ds[0].keys()","metadata":{"id":"6Lm743y2BGVj","outputId":"2e4adfe3-0cd7-4946-d97f-c4b977186bb4","papermill":{"duration":0.029804,"end_time":"2024-04-07T08:22:57.211983","exception":false,"start_time":"2024-04-07T08:22:57.182179","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T17:04:15.422712Z","iopub.execute_input":"2024-04-08T17:04:15.423032Z","iopub.status.idle":"2024-04-08T17:04:15.437572Z","shell.execute_reply.started":"2024-04-08T17:04:15.423004Z","shell.execute_reply":"2024-04-08T17:04:15.436715Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"dict_keys(['document', 'full_text', 'tokens', 'trailing_whitespace', 'input_ids', 'attention_mask', 'offset_mapping', 'token_map'])"},"metadata":{}}]},{"cell_type":"markdown","source":"#  Prediction Processing Function ","metadata":{"id":"a85b8d72","papermill":{"duration":0.011259,"end_time":"2024-04-07T08:22:57.234851","exception":false,"start_time":"2024-04-07T08:22:57.223592","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"🔍 **Explanation:**\n\n- **`process_predictions` Function**:\n  - Takes a list of flattened predictions (`flattened_preds`) as input and applies softmax to each set of predictions.\n- **Initialization**:\n  - Initializes an empty list `predictions_softmax_all` to hold softmax-applied predictions for all sets.\n- **Processing Predictions**:\n  - Iterates over each set of predictions in `flattened_preds`.\n  - For each set of predictions, it applies softmax along the last dimension (dimension `-1`) using `torch.softmax()` to convert logits to probabilities.\n  - The softmax-applied predictions are then appended to the `predictions_softmax_all` list.\n- **Return**:\n  - Returns the list of predictions with softmax applied.\n\n📚 **Study Sources:**\n\n1. PyTorch Documentation - `torch.softmax()`: [https://pytorch.org/docs/stable/generated/torch.softmax.html](https://pytorch.org/docs/stable/generated/torch.softmax.html)","metadata":{}},{"cell_type":"code","source":"def process_predictions(flattened_preds):\n    # Initialize a list to hold softmax-applied predictions\n    predictions_softmax_all = []\n\n    # Iterate over each set of predictions in the input\n    for predictions in flattened_preds:\n        # Apply softmax to convert logits to probabilities\n        predictions_softmax = torch.softmax(predictions, dim=-1)\n        # Append the softmax predictions to the result list\n        predictions_softmax_all.append(predictions_softmax)\n\n    # Return the list of predictions with softmax applied\n    return predictions_softmax_all\n","metadata":{"id":"KEsEyR4myBZ1","papermill":{"duration":0.020727,"end_time":"2024-04-07T08:22:57.267082","exception":false,"start_time":"2024-04-07T08:22:57.246355","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T17:04:15.438898Z","iopub.execute_input":"2024-04-08T17:04:15.439263Z","iopub.status.idle":"2024-04-08T17:04:15.448072Z","shell.execute_reply.started":"2024-04-08T17:04:15.439229Z","shell.execute_reply":"2024-04-08T17:04:15.447290Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"# 🚀Prediction and Conversion Functions \n","metadata":{}},{"cell_type":"markdown","source":"🔍 **Explanation:**\n\n- **Prediction and Conversion Functions**:\n  - Functions to perform prediction, model export to ONNX format, quantization, and inference using ONNX runtime.\n- **`predict_and_convert` Function**:\n  - Exports the PyTorch model to ONNX format with specified configurations and saves it to the specified path.\n- **`predict_and_quant` Function**:\n  - Performs quantization on the original ONNX model and saves the quantized model to the specified path.\n- **`predict` Function**:\n  - Performs inference using the provided ONNX model session over all batches from a data loader and returns processed predictions.\n\n📚 **Study Sources:**\n\n1. ONNX Documentation - Python API Overview: [https://onnxruntime.ai/docs/api/python_api_overview.html](https://onnxruntime.ai/docs/api/python_api_overview.html)\n2. PyTorch Documentation - `torch.onnx.export()`: [https://pytorch.org/docs/stable/generated/torch.onnx.export.html](https://pytorch.org/docs/stable/generated/torch.onnx.export.html)\n3. Hugging Face Transformers Documentation - Model Quantization: [https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.convert_to_onnx](https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.convert_to_onnx)","metadata":{}},{"cell_type":"code","source":"\nimport torch.onnx\nimport onnx\nimport onnxruntime\n\ndef predict_and_convert(data_loader, model, config, onnx_model_path):\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Initialize a list to store the prediction outputs\n    prediction_outputs = []\n\n    # Create an iterator from the DataLoader\n    data_iter = iter(data_loader)\n\n    # Fetch the first batch of data from the iterator\n    batch = next(data_iter)\n\n    # Disable gradient calculations for export\n    with torch.no_grad():\n        # Prepare inputs by reshaping and moving them to the specified device\n        inputs = {key: val.reshape(val.shape[0], -1).to(config.device) for key, val in batch.items() if key in ['input_ids', 'attention_mask']}\n        input_ids = inputs['input_ids']\n        attention_mask = inputs['attention_mask']\n\n        # Export the model to ONNX format with the specified configurations\n        torch.onnx.export(model,  # Model to be exported\n                          args=(input_ids, attention_mask),  # Example model input\n                          f=onnx_model_path,  # Path to save the ONNX model\n                          opset_version=12,  # ONNX opset version\n                          input_names=['input_ids', 'attention_mask'],  # Names of the input parameters\n                          output_names=['logits'],  # Names of the output\n                          dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence_length'},  # Dynamic axes for batching\n                                        'attention_mask': {0: 'batch_size', 1: 'sequence_length'}}\n                          )\n\n    print(\"Model saved to\", onnx_model_path)\n\n    return prediction_outputs\n\ndef predict_and_quant(data_loader, config, original_onnx_model_path, output_file_name, data_path):\n    # Initialize a list to store prediction outputs\n    prediction_outputs = []\n\n    # Create an iterator from the DataLoader\n    data_iter = iter(data_loader)\n\n    # Fetch the first batch of data from the iterator\n    batch = next(data_iter)\n\n    # Disable gradient calculations for efficiency\n    with torch.no_grad():\n        \n        # Prepare inputs by reshaping and moving them to the specified device\n        inputs = {key: val.reshape(val.shape[0], -1).to(config.device) for key, val in batch.items() if key in ['input_ids', 'attention_mask']}\n        \n        input_ids = inputs['input_ids']\n        attention_mask = inputs['attention_mask']\n\n\n        # Prepare input data for quantization by moving tensors to CPU and converting to numpy arrays\n        input_data = {\"input_ids\": input_ids.cpu().numpy(), \"attention_mask\": attention_mask.cpu().numpy()}\n            \n        # Call the function to auto convert the ONNX model to mixed precision with specified settings\n        auto_convert_mixed_precision_model_path(\n            original_onnx_model_path,  # Original ONNX model path\n            input_data,  # Input data for calibration during quantization\n            output_file_name,  # Output file name for the quantized model\n            provider=['CUDAExecutionProvider'],  # Specify the execution provider, can be changed to CPU if necessary\n            location=data_path,  # specify the path to save external data tensors\n            rtol=2,  # Relative tolerance for quantization\n            atol=20,  # Absolute tolerance for quantization\n            keep_io_types=True,  # Maintain input/output types\n            verbose=True  # Enable verbose output during quantization\n        )\n\n        # Append a placeholder value to prediction outputs (currently not used for actual predictions)\n        prediction_outputs.append(0)\n\n    print(\"Model saved to\", output_file_name)\n\n    return prediction_outputs\n\ndef predict(data_loader, session, config):\n\n    # Initialize a list to collect raw predictions for each batch\n    prediction_outputs = []\n\n    # Iterate over all batches of data from the data loader\n    for batch in tqdm(data_loader, desc=\"Predicting\"):\n        with torch.no_grad():\n            # Prepare inputs by reshaping and moving them to the specified device\n            inputs = {key: val.reshape(val.shape[0], -1).to(config.device) for key, val in batch.items() if key in ['input_ids', 'attention_mask']}\n            \n            # Retrieve the names of the input and output nodes from the model session\n            input_names = [inp.name for inp in session.get_inputs()]\n            output_names = [out.name for out in session.get_outputs()]\n            \n            # Extract input_ids and attention_mask from inputs\n            input_ids = inputs['input_ids']\n            attention_mask = inputs['attention_mask']\n\n            # Prepare input data by moving tensors to CPU and converting to numpy arrays\n            input_data = {\"input_ids\": input_ids.cpu().numpy(), \"attention_mask\": attention_mask.cpu().numpy()}\n\n            # Execute the model\n            onnx_outputs = session.run(None, input_data)\n\n            # Append raw model outputs (predictions) to the list\n            prediction_outputs.append(torch.tensor(onnx_outputs[0]))  # Assuming the first output is what we need\n\n    # Flatten the list of predictions across all batches\n    prediction_outputs = [logit for batch in prediction_outputs for logit in batch]\n\n    # Process the predictions as required (e.g., applying softmax, thresholding)\n    processed_predictions = process_predictions(prediction_outputs)\n\n    return processed_predictions\n","metadata":{"papermill":{"duration":1.247612,"end_time":"2024-04-07T08:22:58.526653","exception":false,"start_time":"2024-04-07T08:22:57.279041","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T17:04:15.449463Z","iopub.execute_input":"2024-04-08T17:04:15.449727Z","iopub.status.idle":"2024-04-08T17:04:15.469162Z","shell.execute_reply.started":"2024-04-08T17:04:15.449704Z","shell.execute_reply":"2024-04-08T17:04:15.468177Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"🔍 **Explanation:**\n\n- **Prediction Processing Function with Thresholding**:\n  - Processes the flattened predictions from the model with the specified threshold to determine the final class predictions.\n- **Initialization**:\n  - Initializes an empty list `preds_final` to store the final predictions.\n- **Processing Predictions**:\n  - Iterates over each set of predictions (`flattened_preds`).\n  - Retrieves the softmax-applied predictions.\n  - Determines the argmax prediction across all classes.\n  - Determines predictions for all classes except 'O'.\n  - Retrieves the softmax probabilities for the 'O' class.\n  - Applies the threshold to decide between 'O' class and other classes.\n  - Converts final predictions to a numpy array and appends to `preds_final` list.\n- **Thresholding**:\n  - The specified threshold (`threshold`) is used to determine whether to choose the 'O' class or other classes based on softmax probabilities.\n- **Return**:\n  - Returns the list of final predictions after thresholding.\n\n📚 **Study Sources:**\n- PyTorch Documentation - `torch.where()`: [https://pytorch.org/docs/stable/generated/torch.where.html](https://pytorch.org/docs/stable/generated/torch.where.html)","metadata":{}},{"cell_type":"code","source":"def process_predictions_ans(flattened_preds, threshold=0.95):\n\n    preds_final = []  # Initialize a list to store final predictions\n\n    # Iterate over each set of predictions\n    for predictions in flattened_preds:\n        # softmax was applied to the first dimension before averaging\n        predictions_softmax = predictions\n\n        # Get the argmax across all classes\n        predictions_argmax = predictions.argmax(-1)\n\n        # Get predictions for all classes except 'O'\n        predictions_without_O = predictions_softmax[:, :12].argmax(-1)\n\n        # Get the softmax probabilities for the 'O' class\n        O_predictions = predictions_softmax[:, 12]\n\n        # Apply threshold to decide between 'O' class and other classes\n        pred_final = torch.where(O_predictions < threshold, predictions_without_O, predictions_argmax)\n\n        # Convert final predictions to numpy array and add to the list\n        preds_final.append(pred_final.numpy())\n\n    return preds_final\n","metadata":{"papermill":{"duration":0.021962,"end_time":"2024-04-07T08:22:58.560749","exception":false,"start_time":"2024-04-07T08:22:58.538787","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T17:04:15.470384Z","iopub.execute_input":"2024-04-08T17:04:15.470676Z","iopub.status.idle":"2024-04-08T17:04:15.481042Z","shell.execute_reply.started":"2024-04-08T17:04:15.470651Z","shell.execute_reply":"2024-04-08T17:04:15.480104Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"# 📥 Loading Tokenized Dataset from Disk \n\n","metadata":{}},{"cell_type":"markdown","source":"🔍 **Explanation:**\n\n- **Loading Tokenized Dataset from Disk**:\n  - Loads the tokenized dataset from disk, either for test or training purposes, based on the condition.\n- **Data Preparation**:\n  - Defines the columns to keep (`input_ids`, `attention_mask`) in the dataset.\n  - Initializes a data collator for token classification using the `DataCollatorForTokenClassification` class from the `transformers` library.\n- **Conditional Loading**:\n  - If `debug_on_train_df` is `False`, it loads the test dataset, removes unnecessary columns, updates configuration variables, and creates a DataLoader for the test dataset.\n  - If `debug_on_train_df` is `True`, it loads the dataset for the specified fold, performs similar preprocessing, and creates a DataLoader for the dataset.\n- **DataLoader Parameters**:\n  - `batch_size`: Number of samples per batch during inference.\n  - `shuffle`: Whether to shuffle the data.\n  - `num_workers`: Number of subprocesses for data loading.\n  - `pin_memory`: Whether to pin memory for faster data transfer to GPU.\n- **Return**:\n  - Returns a DataLoader object (`test_dataloader`) for iterating over the test dataset.\n\n📚 **Study Sources:**\n- Hugging Face Documentation - `DataCollatorForTokenClassification`: [https://huggingface.co/transformers/main_classes/data_collator.html#transformers.DataCollatorForTokenClassification](https://huggingface.co/transformers/main_classes/data_collator.html#transformers.DataCollatorForTokenClassification)\n- PyTorch DataLoader Documentation: [https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n```","metadata":{}},{"cell_type":"code","source":"# Columns to keep in the dataset\nkeep_cols = {\"input_ids\", \"attention_mask\"}\n\n# Data collator for token classification\ncollator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=512)\n\n# Load Tokenized Dataset from Disk\nif not debug_on_train_df:\n    # Load test dataset\n    test_ds = load_from_disk(f'{config.save_dir}test.dataset')\n    # Remove unnecessary columns from test dataset\n    test_ds = test_ds.remove_columns([c for c in test_ds.column_names if c not in keep_cols])\n    # Update configuration variables\n    config.data_length = len(test_ds)\n    config.len_token = len(tokenizer)\n    # Create DataLoader for test dataset\n    test_dataloader = DataLoader(test_ds, batch_size=config.batch_size, shuffle=False, num_workers=4, pin_memory=False, collate_fn=collator)\nelse:\n    # Fallback for memory tests of several models on fold zero\n    fold = config.trn_fold\n    # Load dataset for specified fold\n    test_ds = load_from_disk(f'{config.save_dir}fold_{fold}.dataset')\n    # Remove unnecessary columns\n    test_ds = test_ds.remove_columns([c for c in test_ds.column_names if c not in keep_cols])\n    # Update configuration variables\n    config.data_length = len(test_ds)\n    config.len_token = len(tokenizer)\n    # Create DataLoader for test dataset\n    test_dataloader = DataLoader(test_ds, batch_size=config.batch_size, shuffle=False, num_workers=4, pin_memory=False, collate_fn=collator)\n","metadata":{"papermill":{"duration":0.059871,"end_time":"2024-04-07T08:22:58.632531","exception":false,"start_time":"2024-04-07T08:22:58.572660","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T17:04:15.482301Z","iopub.execute_input":"2024-04-08T17:04:15.482797Z","iopub.status.idle":"2024-04-08T17:04:15.673522Z","shell.execute_reply.started":"2024-04-08T17:04:15.482764Z","shell.execute_reply":"2024-04-08T17:04:15.672529Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"🔍 **Explanation:**\n\n- **All Prediction Data Processing**:\n  - Iterates over each model path and its weight defined in `config.model_paths`.\n- **Model Conversion and Quantization**:\n  - If `convert_before_inference` is `True`, it loads the original model, converts it to ONNX format, and performs quantization. Otherwise, it uses already converted models.\n- **ONNX Runtime Session Creation**:\n  - Creates an ONNX Runtime session for GPU execution to perform inference.\n- **Prediction**:\n  - Performs prediction using the specified ONNX model session over the test dataset.\n- **Ensemble Preparation**:\n  - Stores the softmax-applied logits for each model's predictions for potential ensemble learning.\n- **Memory Management**:\n  - Cleans up resources like DataLoaders and datasets after processing predictions to free up memory.\n\n📚 **Study Sources:**\n- ONNX Runtime Documentation - Python API: [https://onnxruntime.ai/docs/api/python_api.html](https://onnxruntime.ai/docs/api/python_api.html)\n- Hugging Face Transformers Documentation - Model Quantization: [https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.convert_to_onnx](https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.convert_to_onnx)\n```","metadata":{}},{"cell_type":"code","source":"# All predict data\npredictions_softmax_logits = []\nall_preds = []\n\nfor model_path, weight in config.model_paths.items():\n    \n    fold = config.trn_fold\n    \n    if convert_before_inference:\n\n        # Loading the original model and converting it to ONNX\n        model = AutoModelForTokenClassification.from_pretrained(model_path)\n\n        # Converting it to ONNX to a temp folder\n        converted_model_name = temp_data_folder + \"original_model.onnx\"\n        predictions_softmax_all = predict_and_convert(test_dataloader, model, config, converted_model_name)\n        del model\n        gc.collect()\n        torch.cuda.empty_cache()\n\n        # In commit mode, save all quantized models with different names to create a dataset and reuse them later bypassing\n        #vquantization and conversion\n        quantized_model_name = \"/kaggle/working/optimized\" + model_path.split(\"/\")[-1] + \"_f\" + str(fold) + \".onnx\"\n        # data path should be relative\n        quantized_data_path = \"optimized\" + model_path.split(\"/\")[-1] + \"_f\" + str(fold) + \".data\"\n        \n        # Quantization\n        predictions_softmax_all = predict_and_quant(test_dataloader, config, converted_model_name, quantized_model_name, quantized_data_path)\n    \n    else:\n        # Use already converted models, you can make a commit notebook once and save output models to a dataset,\n        # for example, /kaggle/input/toonnx2-converted-models    \n        quantized_model_name = config.converted_path + \"/optimized\" + model_path.split(\"/\")[-1] + \"_f\" + str(fold) + \".onnx\"\n\n    \n    # Create ONNX Runtime session for GPU\n    session = onnxruntime.InferenceSession(quantized_model_name, providers=['CUDAExecutionProvider'])\n    # Uncomment this if you want to debug something on CPU\n    # session = onnxruntime.InferenceSession(quantized_model_name)\n    \n    # Predict \n    predictions_softmax_all = predict(test_dataloader, session, config)\n    \n    # Keep all logits for ensemble later\n    predictions_softmax_logits.append(predictions_softmax_all)\n    \ndel test_dataloader, test_ds\ngc.collect()\ntorch.cuda.empty_cache()\n","metadata":{"id":"4ea1ecae","outputId":"e2854da8-6dc9-4c52-fca6-0bc335c3f423","papermill":{"duration":306.406037,"end_time":"2024-04-07T08:28:05.050290","exception":false,"start_time":"2024-04-07T08:22:58.644253","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T17:04:15.675619Z","iopub.execute_input":"2024-04-08T17:04:15.675907Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[0;93m2024-04-08 17:04:16.018952538 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:16.232084188 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:16.297498056 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:16.363338513 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:16.427876770 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:16.492669548 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:16.556942276 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:16.621293403 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:16.685924363 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:16.751655753 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:18.146622107 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 74 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n\u001b[0;93m2024-04-08 17:04:18.184750280 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n\u001b[0;93m2024-04-08 17:04:18.184775245 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Predicting:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"392ae6ac6fae40bf8228e84d8eb368df"}},"metadata":{}},{"name":"stderr","text":"\u001b[0;93m2024-04-08 17:04:22.514912415 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:22.733821291 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:22.802137469 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:22.870469702 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:22.939402814 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:23.010275538 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:23.079616913 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:23.151609480 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:23.223479207 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:23.293512288 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:24.602334530 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 74 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n\u001b[0;93m2024-04-08 17:04:24.638538351 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n\u001b[0;93m2024-04-08 17:04:24.638564881 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Predicting:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6ab0a913c024c1782152b1a9b4be3e5"}},"metadata":{}},{"name":"stderr","text":"\u001b[0;93m2024-04-08 17:04:28.980541430 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:29.199544752 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:29.266780840 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:29.335044272 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:29.402090933 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:29.469343016 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:29.539676858 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:29.607818220 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:29.676057083 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:29.744260270 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:31.151241999 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 74 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n\u001b[0;93m2024-04-08 17:04:31.199119534 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n\u001b[0;93m2024-04-08 17:04:31.199149226 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Predicting:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d75f9d40824467f9147643cf408006a"}},"metadata":{}},{"name":"stderr","text":"\u001b[0;93m2024-04-08 17:04:35.640233096 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:35.852734977 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:35.919422624 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:35.988055351 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:36.055448660 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:36.121915392 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:36.188577394 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:36.275195530 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:36.356934422 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:36.432857984 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:37.822727604 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 74 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n\u001b[0;93m2024-04-08 17:04:37.859809869 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n\u001b[0;93m2024-04-08 17:04:37.859835073 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Predicting:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7258212e4b6447a8f77b192751aecf9"}},"metadata":{}},{"name":"stderr","text":"\u001b[0;93m2024-04-08 17:04:42.271929993 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:42.488129332 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:42.557507134 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:42.626823635 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:42.696485725 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:42.767652223 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:42.837271042 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:42.905888797 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:42.974744874 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:43.045467521 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:44.375458129 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 74 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n\u001b[0;93m2024-04-08 17:04:44.414166506 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n\u001b[0;93m2024-04-08 17:04:44.414193416 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Predicting:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc25a58960e7404fb28cb8d877208dd3"}},"metadata":{}},{"name":"stderr","text":"\u001b[0;93m2024-04-08 17:04:48.793299762 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:49.009870735 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:49.078517873 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:49.145730491 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:49.217080590 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:49.289509617 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:49.356518764 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:49.424235978 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:49.492133324 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:49.561015979 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:50.975632653 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 74 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n\u001b[0;93m2024-04-08 17:04:51.015376156 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n\u001b[0;93m2024-04-08 17:04:51.015401731 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Predicting:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6994280cbc8b4fd38961339f80047d44"}},"metadata":{}},{"name":"stderr","text":"\u001b[0;93m2024-04-08 17:04:55.390834488 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:55.611245993 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:55.680525403 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:55.750711142 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:55.821019267 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:55.889413304 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:55.960102680 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:56.029349152 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:56.096301153 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:56.164867607 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:04:57.600408051 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 74 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n\u001b[0;93m2024-04-08 17:04:57.643387053 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n\u001b[0;93m2024-04-08 17:04:57.643413839 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7e14bbd2dbd0>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n    self._shutdown_workers()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1442, in _shutdown_workers\n    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 149, in join\n    res = self._popen.wait(timeout)\n  File \"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n    if not wait([self.sentinel], timeout):\n  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n    ready = selector.select(timeout)\n  File \"/opt/conda/lib/python3.10/selectors.py\", line 416, in select\n    fd_event_list = self._selector.poll(timeout)\nKeyboardInterrupt: \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Predicting:   0%|          | 0/10 [01:11<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5e9685232be4d3c88983ad2cb39fcfb"}},"metadata":{}},{"name":"stderr","text":"\u001b[0;93m2024-04-08 17:06:13.019665659 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:06:13.237867552 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:06:13.307274846 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:06:13.375342537 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:06:13.443842577 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:06:13.513053704 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:06:13.583333320 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:06:13.651752829 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:06:13.720260664 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:06:13.793754287 [W:onnxruntime:, constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold ReduceMean node '/deberta/encoder/LayerNorm/ReduceMean'\u001b[m\n\u001b[0;93m2024-04-08 17:06:15.153080512 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 74 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n\u001b[0;93m2024-04-08 17:06:15.192890317 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n\u001b[0;93m2024-04-08 17:06:15.192916357 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Predicting:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa7aa7b9cd1e4b2f9b6b63f0a428ff36"}},"metadata":{}}]},{"cell_type":"markdown","source":"🔍 **Explanation:**\n\n- **Calculating Weighted Mean of Predictions**:\n  - Calculates the weighted mean of softmax predictions from all models.\n- **Total Weight Calculation**:\n  - Calculates the total weight of all models specified in `config.model_paths` to normalize the weights if their sum exceeds 1.\n- **Individual Model Weights**:\n  - Retrieves the individual weights for each model.\n- **Sample-wise Computation**:\n  - Iterates over each sample's predictions since the length of texts can vary.\n- **Weighted Prediction Accumulation**:\n  - For each sample, it initializes a tensor to accumulate weighted predictions from all models.\n  - Iterates over each model to compute its contribution to the final prediction, applying relative weights.\n  - Weighted predictions are added to obtain the sum.\n- **Mean Calculation**:\n  - Appends the mean of the weighted predictions for the current sample to the list `predictions_mean_all`.\n\n📚 **Study Sources:**\n- PyTorch Documentation - Tensor Operations: [https://pytorch.org/docs/stable/tensors.html](https://pytorch.org/docs/stable/tensors.html)\n```","metadata":{"papermill":{"duration":0.014547,"end_time":"2024-04-07T08:28:06.231889","exception":false,"start_time":"2024-04-07T08:28:06.217342","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Initialize an empty list to store the mean of the softmax predictions from all models.\npredictions_mean_all = []\n\n# Calculate the total weight of all models to normalize the weights if its sum exceeds 1.\ntotal_weight = sum(config.model_paths.values())\nprint(f\"Total weight: {total_weight}\")\n\n# Retrieve the individual weights for each model.\nmodel_weights = list(config.model_paths.values())\n\n# Iterate over each sample since the length of texts can vary.\nfor sample_index in range(len(predictions_softmax_logits[0])):\n    \n    # Initialize a tensor to accumulate weighted predictions for the current sample.\n    weighted_predictions_sum = torch.zeros(predictions_softmax_logits[0][sample_index].size())\n\n    # Iterate over each model to compute its contribution to the final prediction.\n    for model_index in range(len(predictions_softmax_logits)):\n        weighted_prediction = predictions_softmax_logits[model_index][sample_index] * (model_weights[model_index] / total_weight)\n        weighted_predictions_sum += weighted_prediction\n\n    # Append the mean of the weighted predictions for the current sample to the list.\n    predictions_mean_all.append(weighted_predictions_sum)\n","metadata":{"papermill":{"duration":0.04469,"end_time":"2024-04-07T08:28:06.291560","exception":false,"start_time":"2024-04-07T08:28:06.246870","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n# 🔄Processing Final Predictions \n\n🔍 **Explanation:**\n\n- **Processing Final Predictions**:\n  - Processes the final predictions using the previously defined function `process_predictions_ans`.\n- **Function Call**:\n  - Calls the `process_predictions_ans` function, passing the mean of the softmax predictions from all models (`predictions_mean_all`) as input.\n- **Processed Predictions**:\n  - The processed predictions are stored in the variable `processed_predictions` for further analysis or evaluation.\n\n📚 **Study Sources:**\n- Official Python Documentation - Function Definitions: [https://docs.python.org/3/tutorial/controlflow.html#defining-functions](https://docs.python.org/3/tutorial/controlflow.html#defining-functions)\n```","metadata":{}},{"cell_type":"code","source":"processed_predictions = process_predictions_ans(predictions_mean_all)\n","metadata":{"papermill":{"duration":0.03836,"end_time":"2024-04-07T08:28:06.344810","exception":false,"start_time":"2024-04-07T08:28:06.306450","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Processing Predictions and Extracting Information 🔍\n\n🔍 **Explanation:**\n\n- **Processing Predictions and Extracting Information**:\n  - Iterates over each prediction and its corresponding token mapping, offsets, tokens, document, and full text in the dataset.\n- **Token-level Processing**:\n  - Iterates through each token prediction and its offsets, adjusting for trailing whitespace if necessary, and checks if it's a valid token. If so, it adds the token information to the processed list.\n- **Extracting Structured Data**:\n  - Extracts email addresses, phone numbers, and URLs from the full text using regular expressions and stores them along with their corresponding document, token index, predicted label, and token string.\n- **Efficient Membership Check**:\n  - The set `pairs` is used to efficiently check whether a pair (document, token_id) has been processed already, reducing duplicate processing.\n  \n📚 **Study Sources:**\n- Python Documentation - Regular Expression Operations: [https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html)\n- Python Documentation - Sets: [https://docs.python.org/3/tutorial/datastructures.html#sets](https://docs.python.org/3/tutorial/datastructures.html#sets)\n```","metadata":{}},{"cell_type":"code","source":"# Initialize empty lists and sets to store extracted information\ntriplets = []  # Triplets for structured data extraction\npairs = set()  # To efficiently check membership during processing\nprocessed = []  # Processed predictions\nemails = []  # Email addresses\nphone_nums = []  # Phone numbers\nurls = []  # URLs\nstreets = []  # Street addresses (if applicable)\n\n# Iterate over each prediction, token mapping, offsets, tokens, and document in the dataset\nfor p, token_map, offsets, tokens, doc, full_text in zip(\n    processed_predictions, \n    ds[\"token_map\"], \n    ds[\"offset_mapping\"], \n    ds[\"tokens\"], \n    ds[\"document\"],\n    ds[\"full_text\"]\n):\n\n    # Iterate through each token prediction and its corresponding offsets\n    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n        label_pred = id2label[token_pred]  # Predicted label from token\n        \n        # Skip tokens if start and end index are both zero or label is O\n        if start_idx + end_idx == 0 or label_pred == \"O\":\n            continue\n        \n        # Adjust start index if there's trailing whitespace\n        if token_map[start_idx] == -1:\n            start_idx += 1\n        while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n            start_idx += 1\n        if start_idx >= len(token_map):\n            break\n        \n        # Get the token ID at start index\n        token_id = token_map[start_idx]\n        \n        # Skip if label is O or it's not a valid token ID\n        if label_pred in (\"O\", \"B-EMAIL\", \"B-PHONE_NUM\", \"I-PHONE_NUM\") or token_id == -1:\n            continue\n        \n        # Create a unique pair (document, token_id) and check if it's not already processed\n        pair = (doc, token_id)\n        if pair not in pairs:\n            processed.append({\"document\": doc, \"token\": token_id, \"label\": label_pred, \"token_str\": tokens[token_id]})\n            pairs.add(pair)\n    \n    # Extract email addresses\n    for token_idx, token in enumerate(tokens):\n        if re.fullmatch(email_regex, token) is not None:\n            emails.append(\n                {\"document\": doc, \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n            )\n                \n    # Extract phone numbers\n    matches = phone_num_regex.findall(full_text)\n    if matches:\n        for match in matches:\n            target = [t.text for t in nlp.tokenizer(match)]\n            matched_spans = find_span(target, tokens)\n            for matched_span in matched_spans:\n                for intermediate, token_idx in enumerate(matched_span):\n                    prefix = \"I\" if intermediate else \"B\"\n                    phone_nums.append(\n                        {\"document\": doc, \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": tokens[token_idx]}\n                    )\n    \n    # Extract URLs\n    matches = url_regex.findall(full_text)\n    if matches:\n        for match in matches:\n            target = [t.text for t in nlp.tokenizer(match)]\n            matched_spans = find_span(target, tokens)\n            for matched_span in matched_spans:\n                for intermediate, token_idx in enumerate(matched_span):\n                    prefix = \"I\" if intermediate else \"B\"\n                    urls.append(\n                        {\"document\": doc, \"token\": token_idx, \"label\": f\"{prefix}-URL_PERSONAL\", \"token_str\": tokens[token_idx]}\n                    )\n","metadata":{"papermill":{"duration":0.133881,"end_time":"2024-04-07T08:28:06.493448","exception":false,"start_time":"2024-04-07T08:28:06.359567","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 📝 Creating submission.CSV \n","metadata":{}},{"cell_type":"code","source":"\n# Create a DataFrame from processed data, phone numbers, emails, and URLs\ndf = pd.DataFrame(processed + phone_nums + emails + urls)\n\n# Assign each row a unique 'row_id'\ndf[\"row_id\"] = list(range(len(df)))\n\n# Export the DataFrame to a CSV file for further exploration\ndf[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)\n","metadata":{"papermill":{"duration":0.046327,"end_time":"2024-04-07T08:28:06.555293","exception":false,"start_time":"2024-04-07T08:28:06.508966","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Keep Exploring! 👀\n\nThank you for delving into this notebook! If you found it insightful or beneficial, I encourage you to explore more of my projects and contributions on my profile.\n\n👉 [Visit my Profile](https://www.kaggle.com/zulqarnainalipk) 👈\n\n[GitHub]( https://github.com/zulqarnainalipk) |\n[LinkedIn]( https://www.linkedin.com/in/zulqarnainalipk/)\n\n## Share Your Thoughts! 🙏\n\nYour feedback is invaluable! Your insights and suggestions drive our ongoing improvement. If you have any comments, questions, or ideas to contribute, please feel free to reach out.\n\n📬 Contact me via email: [zulqar445ali@gmail.com](mailto:zulqar445ali@gmail.com)\n\nI extend my sincere gratitude for your time and engagement. Your support inspires us to create even more valuable content.\nHappy coding and best of luck in your data science endeavors! 🚀\n","metadata":{}},{"cell_type":"markdown","source":"#PRINT OK","metadata":{}}]}